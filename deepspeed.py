# -*- coding: utf-8 -*-
"""DeepSpeed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XltLR2I9LHBvzYoaktVC-ZP5_KzWFqpC
"""

# Define DeepSpeed configuration
deepspeed_config = {
    "train_batch_size": 32,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 0.0001,
            "betas": [0.9, 0.999],
            "eps": 1e-8,
            "weight_decay": 1e-6
        }
    },
    "fp16": {
        "enabled": False
    }
}

# Save the DeepSpeed configuration to a file
with open('deepspeed_config.json', 'w') as f:
    json.dump(deepspeed_config, f)

# Load data from Excel file
try:
    data = pd.read_excel('/content/sample.xlsx')
except Exception as e:
    print(f"Error loading data: {e}")
    exit()

time_col = data.iloc[:, 0].values
rainfall = data['Rainfall'].values

# Normalize rainfall data
scaler = MinMaxScaler()
rainfall_normalized = scaler.fit_transform(rainfall.reshape(-1, 1)).flatten()

# Split the data into training, validation, and testing sets (70% train, 15% val, 15% test)
train_size = int(0.7 * len(data))
val_size = int(0.15 * len(data))
test_size = len(data) - train_size - val_size

train_time, val_time, test_time = time_col[:train_size], time_col[train_size:train_size + val_size], time_col[train_size + val_size:]
train_rainfall, val_rainfall, test_rainfall = rainfall_normalized[:train_size], rainfall_normalized[train_size:train_size + val_size], rainfall_normalized[train_size + val_size:]

# Define a PyTorch dataset
class TimeSeriesDataset(Dataset):
    def __init__(self, rainfall, seq_length):
        self.rainfall = rainfall
        self.seq_length = seq_length

    def __len__(self):
        return len(self.rainfall) - self.seq_length

    def __getitem__(self, idx):
        return torch.tensor(self.rainfall[idx:idx+self.seq_length], dtype=torch.float32), torch.tensor(self.rainfall[idx+self.seq_length], dtype=torch.float32)

# Define the LSTM model with Dropout layer
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(-1)
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])
        return out

# Hyperparameters
input_size = 1
hidden_size = 128
num_layers = 2
seq_length = 20
batch_size = 16
num_epochs = 10
dropout = 0.1

# Create datasets and data loaders
train_dataset = TimeSeriesDataset(train_rainfall, seq_length)
val_dataset = TimeSeriesDataset(val_rainfall, seq_length)
test_dataset = TimeSeriesDataset(test_rainfall, seq_length)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True if device == 'cuda' else False)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True if device == 'cuda' else False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True if device == 'cuda' else False)

# Create an instance of the model
model = LSTMModel(input_size, hidden_size, num_layers, dropout).to(device)

# Initialize the optimizer
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-7)

# Define the loss functions
mse_criterion = nn.MSELoss()
mae_criterion = nn.L1Loss()

# Load DeepSpeed configuration
deepspeed_config_path = "deepspeed_config.json"

# Initialize DeepSpeed
model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, model_parameters=model.parameters(), training_data=train_dataset, config_params=json.load(open(deepspeed_config_path)))

# Lists to store losses
train_mse_losses = []
train_mae_losses = []
val_mse_losses = []
val_mae_losses = []
test_mse_losses = []
test_mae_losses = []
r2_scores = []


# Training loop with metrics collection
for epoch in range(num_epochs):
    model.train()
    epoch_train_mse_loss = 0.0
    epoch_train_mae_loss = 0.0
    epoch_val_mse_loss = 0.0
    epoch_val_mae_loss = 0.0
    epoch_test_mse_loss = 0.0
    epoch_test_mae_loss = 0.0
    all_test_targets = []
    all_test_outputs = []
    samples_processed = 0
    total_samples = 0
    total_time = 0

    epoch_start_time = time.time()

    # Training Phase
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        optimizer.zero_grad()
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        mse_loss = mse_criterion(outputs.squeeze(), targets)
        mae_loss = mae_criterion(outputs.squeeze(), targets)
        loss = mse_loss + mae_loss
        model.backward(loss)
        model.step()

        total_samples += inputs.size(0)

        epoch_train_mse_loss += mse_loss.item()
        epoch_train_mae_loss += mae_loss.item()
        samples_processed += inputs.size(0)

    # Validation Phase
    model.eval()
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            mse_loss = mse_criterion(outputs.squeeze(), targets)
            mae_loss = mae_criterion(outputs.squeeze(), targets)
            epoch_val_mse_loss += mse_loss.item()
            epoch_val_mae_loss += mae_loss.item()

    # Test Phase (added after validation phase)
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            mse_loss = mse_criterion(outputs.squeeze(), targets)
            mae_loss = mae_criterion(outputs.squeeze(), targets)
            epoch_test_mse_loss += mse_loss.item()
            epoch_test_mae_loss += mae_loss.item()
            all_test_targets.extend(targets.cpu().numpy())
            all_test_outputs.extend(outputs.squeeze().cpu().numpy().flatten())

    # Calculate R² score
    r2 = r2_score(all_test_targets, all_test_outputs)
    r2_scores.append(r2)

    # Average losses
    train_mse_losses.append(epoch_train_mse_loss / len(train_loader))
    train_mae_losses.append(epoch_train_mae_loss / len(train_loader))
    val_mse_losses.append(epoch_val_mse_loss / len(val_loader))
    val_mae_losses.append(epoch_val_mae_loss / len(val_loader))
    test_mse_losses.append(epoch_test_mse_loss / len(test_loader))
    test_mae_losses.append(epoch_test_mae_loss / len(test_loader))

      # Print epoch summary
    print(f'Epoch [{epoch+1}/{num_epochs}], Time: {epoch_duration:.2f}s, '
          f'Train MSE: {train_mse_losses[-1]:.6f}, Train MAE: {train_mae_losses[-1]:.6f}, '
          f'Val MSE: {val_mse_losses[-1]:.6f}, Val MAE: {val_mae_losses[-1]:.6f}, '
          f'Test MSE: {test_mse_losses[-1]:.6f}, Test MAE: {test_mae_losses[-1]:.6f}, R² Score: {r2:.6f}')



# Prepare data for saving to Excel
loss_data = {
    'Epoch': list(range(1, num_epochs + 1)),
    'Train MSE': train_mse_losses,
    'Train MAE': train_mae_losses,
    'Val MSE': val_mse_losses,
    'Val MAE': val_mae_losses,
    'Test MSE': test_mse_losses,
    'Test MAE': test_mae_losses,
    'R2 Score': r2_scores
}

# Create a DataFrame
loss_df = pd.DataFrame(loss_data)

print(f"Training, validation, and test losses along with R² saved to {output_file}.")

# Predicting for the next 7 days
future_inputs = torch.tensor(test_rainfall[-seq_length:], dtype=torch.float32).unsqueeze(0).to(device)
future_predictions = []
latency_times = []
with torch.no_grad():
    for i in range(7):
        # Start timer for latency measurement
        start_time = time.time()

        # Prediction step
        future_output = model(future_inputs)
        future_predictions.append(future_output.item())

        # Stop timer and calculate latency for this step
        end_time = time.time()
        latency = end_time - start_time
        latency_times.append(latency)

        # Print individual latency for this prediction
        print(f"Prediction {i+1} Latency: {latency:.6f} seconds")

        # Update future inputs
        future_output = future_output.squeeze(1)
        future_inputs = torch.cat((future_inputs, future_output.unsqueeze(1)), dim=1)


# Denormalize the predictions
future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1)).flatten()

print("Predicted rainfall for the next 7 days:")
print(future_predictions)