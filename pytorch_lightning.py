# -*- coding: utf-8 -*-
"""PyTorch Lightning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iEMI4tGXeIUjHK9E62BBZS-lpqr2QGdF
"""

# Load data from Excel file
try:
    data = pd.read_excel('/content/sample.xlsx')
    time_series = data.iloc[:, 0].values
    rainfall = data.iloc[:, 1].values
except Exception as e:
    print(f"Error loading data: {e}")
    exit()


# Normalize rainfall data
scaler = MinMaxScaler()
rainfall_normalized = scaler.fit_transform(rainfall.reshape(-1, 1)).flatten()

# Split the data into 70% training, 15% validation, and 15% testing sets
train_size = int(0.7 * len(data))
val_size = int(0.15 * len(data))
test_size = len(data) - train_size - val_size

train_time_series = time_series[:train_size]
val_time_series = time_series[train_size:train_size + val_size]
test_time_series = time_series[train_size + val_size:]

train_rainfall = rainfall_normalized[:train_size]
val_rainfall = rainfall_normalized[train_size:train_size + val_size]
test_rainfall = rainfall_normalized[train_size + val_size:]

# Define a PyTorch dataset
class TimeSeriesDataset(Dataset):
    def __init__(self, rainfall, seq_length):
        self.rainfall = rainfall
        self.seq_length = seq_length

    def __len__(self):
        return len(self.rainfall) - self.seq_length

    def __getitem__(self, idx):
        return torch.tensor(self.rainfall[idx:idx+self.seq_length], dtype=torch.float32), torch.tensor(self.rainfall[idx+self.seq_length], dtype=torch.float32)

# Define lists to store losses for each epoch
train_mae_losses = []
train_mse_losses = []
val_mae_losses = []
val_mse_losses = []
test_mae_losses = []
test_mse_losses = []
r2_scores = []

# Define the LSTM model with PyTorch Lightning
class LSTMModel(pl.LightningModule):
    def __init__(self, input_size, hidden_size, num_layers, seq_length, dropout):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)
        self.seq_length = seq_length
        self.dropout = dropout
        self.epoch_start_time = None
        self.total_samples = 0

    def forward(self, x):
        lstm_out, _ = self.lstm(x.unsqueeze(-1))
        out = self.fc(lstm_out[:, -1, :])
        return out

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=0.0001, weight_decay=1e-6)
        return optimizer

    def training_step(self, batch, batch_idx):
        inputs, targets = batch
        outputs = self(inputs)
        mse_loss = nn.MSELoss()(outputs.squeeze(), targets)
        mae_loss = nn.L1Loss()(outputs.squeeze(), targets)
        loss = mse_loss + mae_loss

        train_mse_losses.append(mse_loss.item())
        train_mae_losses.append(mae_loss.item())

        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        inputs, targets = batch
        outputs = self(inputs)
        mse_loss = nn.MSELoss()(outputs.squeeze(), targets)
        mae_loss = nn.L1Loss()(outputs.squeeze(), targets)
        loss = mse_loss + mae_loss

        val_mse_losses.append(mse_loss.item())
        val_mae_losses.append(mae_loss.item())

        self.log('val_loss', loss)
        return loss

# Custom callback to log test losses
class TestLossLogger(pl.Callback):
    def __init__(self, test_loader):
        self.test_loader = test_loader

    def on_validation_epoch_end(self, trainer, pl_module):
        pl_module.eval()
        test_mse_loss_total = 0
        test_mae_loss_total = 0
        all_targets = []
        all_predictions = []
        num_batches = len(self.test_loader)

        with torch.no_grad():
            for inputs, targets in self.test_loader:
                inputs, targets = inputs.to(pl_module.device), targets.to(pl_module.device)
                outputs = pl_module(inputs)
                mse_loss = nn.MSELoss()(outputs.squeeze(), targets)
                mae_loss = nn.L1Loss()(outputs.squeeze(), targets)

                test_mse_loss_total += mse_loss.item()
                test_mae_loss_total += mae_loss.item()

                all_targets.extend(targets.cpu().numpy())
                all_predictions.extend(outputs.squeeze().cpu().numpy())

        avg_mse_loss = test_mse_loss_total / num_batches
        avg_mae_loss = test_mae_loss_total / num_batches
        r2 = r2_score(all_targets, all_predictions)

        test_mse_losses.append(avg_mse_loss)
        test_mae_losses.append(avg_mae_loss)
        r2_scores.append(r2)


# Hyperparameters
input_size = 1
hidden_size = 128
num_layers = 2
seq_length = 20
batch_size = 32
num_epochs = 10
dropout = 0.2

# Create datasets and data loaders
train_dataset = TimeSeriesDataset(train_rainfall, seq_length)
val_dataset = TimeSeriesDataset(val_rainfall, seq_length)
test_dataset = TimeSeriesDataset(test_rainfall, seq_length)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Initialize the PyTorch Lightning trainer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')
num_devices = 1 if str(device) == 'cpu' else torch.cuda.device_count()

model = LSTMModel(input_size, hidden_size, num_layers, seq_length, dropout)
test_logger = TestLossLogger(test_loader)
trainer = pl.Trainer(max_epochs=num_epochs, accelerator='cpu' if str(device) == 'cpu' else 'gpu', devices=num_devices, callbacks=[test_logger])

# Training loop
trainer.fit(model, train_loader, val_dataloaders=val_loader)


# Save training, validation, and test losses to Excel
epochs = list(range(1, num_epochs + 1))
losses_df = pd.DataFrame({
    'Epoch': epochs,
    'Train_MAE_Loss': train_mae_losses,
    'Train_MSE_Loss': train_mse_losses,
    'Val_MAE_Loss': val_mae_losses,
    'Val_MSE_Loss': val_mse_losses,
    'Test_MAE_Loss': test_mae_losses,
    'Test_MSE_Loss': test_mse_losses,
    'R2_Score': r2_scores
})

print("Each type of loss and R^2 score.")

# Predicting for the next 7 days
model = model.to(device)
model.eval()

future_inputs = torch.tensor(test_rainfall[-seq_length:], dtype=torch.float32).unsqueeze(0).to(device)
future_predictions = []

with torch.no_grad():
    for i in range(7):
        future_output = model(future_inputs)
        future_predictions.append(future_output.item())

        # Prepare input for next prediction
        future_output = future_output.squeeze(1)
        future_inputs = torch.cat((future_inputs, future_output.unsqueeze(1)), dim=1)

# Denormalize the predictions
future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1)).flatten()

# Create day numbers for future predictions
future_time = np.arange(len(test_time), len(test_time) + 7)

# Print final predicted rainfall values
print("Predicted rainfall for the next 7 days:")
print(future_predictions)