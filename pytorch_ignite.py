# -*- coding: utf-8 -*-
"""PyTorch Ignite.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ECxF586eGvbDeRwUOoFcyZt8qTRuIuxL
"""

# Custom Mean Absolute Error Metric
class MeanAbsoluteError(Metric):
    def __init__(self, output_transform=lambda x: x, device=None):
        super(MeanAbsoluteError, self).__init__(output_transform=output_transform, device=device)
        self._sum_of_absolute_errors = 0.0
        self._num_examples = 0

    def reset(self):
        self._sum_of_absolute_errors = 0.0
        self._num_examples = 0

    def update(self, output):
        y_pred, y = output
        absolute_errors = torch.abs(y_pred - y)
        self._sum_of_absolute_errors += torch.sum(absolute_errors).item()
        self._num_examples += y.shape[0]

    def compute(self):
        return self._sum_of_absolute_errors / self._num_examples


# Load and preprocess data
data = pd.read_excel('/content/sample.xlsx')  # Update the file path as needed
data['Rainfall'] = pd.to_numeric(data.iloc[:, 1], errors='coerce')
data['Rainfall'].fillna(data['Rainfall'].mean(), inplace=True)

time_series = data.iloc[:, 0].values
rainfall = data['Rainfall'].values

# Normalize the data
scaler = MinMaxScaler()
rainfall_normalized = scaler.fit_transform(rainfall.reshape(-1, 1)).flatten()

# Split data: 75% train, 15% validation, 15% test
train_size = int(0.75 * len(rainfall_normalized))
val_size = int(0.15 * len(rainfall_normalized))
train_rainfall = rainfall_normalized[:train_size]
val_rainfall = rainfall_normalized[train_size:train_size + val_size]
test_rainfall = rainfall_normalized[train_size + val_size:]

# Define a PyTorch dataset
class TimeSeriesDataset(Dataset):
    def __init__(self, rainfall, seq_length):
        self.rainfall = rainfall
        self.seq_length = seq_length

    def __len__(self):
        return len(self.rainfall) - self.seq_length

    def __getitem__(self, idx):
        return torch.tensor(self.rainfall[idx:idx+self.seq_length], dtype=torch.float32), torch.tensor(self.rainfall[idx+self.seq_length], dtype=torch.float32)

# Create datasets and data loaders
seq_length = 20
batch_size = 16
train_dataset = TimeSeriesDataset(train_rainfall, seq_length)
val_dataset = TimeSeriesDataset(val_rainfall, seq_length)
test_dataset = TimeSeriesDataset(test_rainfall, seq_length)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define the model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, seq_length, dropout):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)
        self.seq_length = seq_length

    def forward(self, x):
        lstm_out, _ = self.lstm(x.unsqueeze(-1))
        out = self.fc(lstm_out[:, -1, :])
        return out

# Hyperparameters
input_size = 1
hidden_size = 128
num_layers = 2
dropout = 0.1
num_epochs = 10
weight_decay = 1e-7
# Initialize the model
model = LSTMModel(input_size, hidden_size, num_layers, seq_length, dropout).to(device)

# Define loss and optimizer with L2 regularization
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=weight_decay)

# Define the training step
def train_step(engine, batch):
    model.train()
    optimizer.zero_grad()
    inputs, targets = batch
    inputs, targets = inputs.to(device), targets.to(device)
    outputs = model(inputs)
    loss = criterion(outputs.squeeze(), targets)
    loss.backward()
    optimizer.step()
    return loss.item()

# Define the evaluation step
def eval_step(engine, batch):
    model.eval()
    with torch.no_grad():
        inputs, targets = batch
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        return outputs.squeeze(), targets

# Create Ignite engines for training and evaluation
trainer = Engine(train_step)
evaluator = Engine(eval_step)

# Attach MSE and custom MAE metrics to the evaluator
mse = MeanSquaredError(device=device)
mae = MeanAbsoluteError(device=device)
mse.attach(evaluator, "mse")
mae.attach(evaluator, "mae")

# Progress bar
ProgressBar().attach(trainer)
ProgressBar().attach(evaluator)


# Initialize lists to store results for Excel and printing
training_results = []
validation_results = []
testing_results = []

# Event handler for tracking and printing losses
@trainer.on(Events.EPOCH_COMPLETED)
def log_training_results(engine):
    evaluator.run(train_loader)
    metrics = evaluator.state.metrics
    training_results.append((engine.state.epoch, metrics['mse'], metrics['mae']))
    elapsed_time = (datetime.now() - engine.state.start_time).total_seconds()
    num_examples = len(train_loader.dataset)
    print(f'Epoch {engine.state.epoch}: MSE: {metrics["mse"]:.4f}, MAE: {metrics["mae"]:.4f}')

# Event handler for validation results
@trainer.on(Events.EPOCH_COMPLETED)
def log_validation_results(engine):
    evaluator.run(val_loader)
    metrics = evaluator.state.metrics
    validation_results.append((engine.state.epoch, metrics['mse'], metrics['mae']))
    print(f'Epoch {engine.state.epoch}: Validation MSE: {metrics["mse"]:.4f}, MAE: {metrics["mae"]:.4f}')

# Event handler for testing results and R^2 score
@trainer.on(Events.EPOCH_COMPLETED)
def log_testing_results(engine):
    model.eval()
    with torch.no_grad():
        all_predictions = []
        all_actuals = []
        for batch in test_loader:
            inputs, targets = batch
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            all_predictions.extend(outputs.squeeze().cpu().numpy())
            all_actuals.extend(targets.cpu().numpy())

        # Calculate R^2 score
        r2 = r2_score(all_actuals, all_predictions)
        evaluator.run(test_loader)
        metrics = evaluator.state.metrics
        testing_results.append((engine.state.epoch, metrics['mse'], metrics['mae'], r2))
        print(f'Epoch {engine.state.epoch}: Testing MSE: {metrics["mse"]:.4f}, MAE: {metrics["mae"]:.4f}, R^2 Score: {r2:.4f}')



# Predicting for the next 7 days
model = model.to(device)
model.eval()

future_inputs = torch.tensor(test_rainfall[-seq_length:], dtype=torch.float32).unsqueeze(0).to(device)
future_predictions = []

with torch.no_grad():
    for i in range(7):
        future_output = model(future_inputs)
        future_predictions.append(future_output.item())

        # Prepare input for next prediction
        future_output = future_output.squeeze(1)
        future_inputs = torch.cat((future_inputs, future_output.unsqueeze(1)), dim=1)

# Denormalize the predictions
future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1)).flatten()

# Create day numbers for future predictions
future_time = np.arange(len(test_time), len(test_time) + 7)

# Print final predicted rainfall values
print("Predicted rainfall for the next 7 days:")
print(future_predictions)